{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b84805",
   "metadata": {},
   "source": [
    "# Analyzing Wikipedia Clickstream Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddc058",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33467f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2313603",
   "metadata": {},
   "source": [
    "## Introduction to Clickstream Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13837f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list of sample clickstream count\n",
    "sample_clickstream_counts = [\n",
    "    [\"other-search\", \"Hanging_Gardens_of_Babylon\", \"external\", 47000],\n",
    "    [\"other-empty\", \"Hanging_Gardens_of_Babylon\", \"external\", 34600],\n",
    "    [\"Wonders_of_the_World\", \"Hanging_Gardens_of_Babylon\", \"link\", 14000],\n",
    "    [\"Babylon\", \"Hanging_Gardens_of_Babylon\", \"link\", 2500]\n",
    "]\n",
    "\n",
    "clickstream_counts_rdd = spark.sparkContext.parallelize(\n",
    "    sample_clickstream_counts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the RDD of sample clickstream counts\n",
    "clickstream_sample_df = clickstream_counts_rdd\\\n",
    "    .toDF([\"source_page\", \"target_page\",  \"link_category\", \"link_count\"])\n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "clickstream_sample_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268c0b0",
   "metadata": {},
   "source": [
    "## Inspecting Clickstream Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the target directory (`./cleaned/clickstream/`) into a DataFrame (`clickstream`)\n",
    "clickstream = spark.read \\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', '\\t') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"clickstream\")\n",
    "\n",
    "# Display the DataFrame to the notebook\n",
    "clickstream.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934cc169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the schema of the `clickstream` DataFrame to the notebook\n",
    "clickstream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop target columns\n",
    "clickstream = clickstream.drop(\"language_code\")\n",
    "\n",
    "# Display the first few rows of the DataFrame and the new schema in the notebook\n",
    "clickstream.show(5, truncate=False)\n",
    "clickstream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b75baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename `referrer` and `resource` to `source_page` and `target_page`\n",
    "clickstream = clickstream\\\n",
    "    .withColumnRenamed(\"referrer\", \"source_page\")\\\n",
    "    .withColumnRenamed(\"resource\", \"target_page\")\n",
    "  \n",
    "# Display the first few rows of the DataFrame and the new schema in the notebook\n",
    "clickstream.show(5, truncate=False)\n",
    "clickstream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ce589",
   "metadata": {},
   "source": [
    "## Querying Clickstream Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view in the metadata for this `SparkSession` to make the data\n",
    "# queryable with `sparkSession.sql()`\n",
    "clickstream.createOrReplaceTempView(\"clickstream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c20a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and sort the DataFrame using PySpark DataFrame methods\n",
    "clickstream\\\n",
    "    .filter(clickstream.target_page == 'Hanging_Gardens_of_Babylon')\\\n",
    "    .orderBy('click_count', ascending=False)\\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bbec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter and sort the DataFrame using SQL\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM clickstream\n",
    "    WHERE target_page = 'Hanging_Gardens_of_Babylon'\n",
    "    ORDER BY click_count DESC\n",
    "    \"\"\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bac86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the DataFrame using PySpark DataFrame Methods \n",
    "clickstream\\\n",
    "    .groupBy('link_category')\\\n",
    "    .sum()\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ff99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the DataFrame using SQL\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT link_category, SUM(click_count) FROM clickstream\n",
    "    GROUP BY link_category\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b756c1",
   "metadata": {},
   "source": [
    "## Saving Results to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e74d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame (named `internal_clickstream`) using `filter` to select rows to \n",
    "# a specific condition and `select` to choose which columns to return from the query.\n",
    "internal_clickstream = clickstream\\\n",
    "    .select([\"source_page\", \"target_page\", \"click_count\"])\\\n",
    "    .filter(clickstream.link_category == 'link')\n",
    "\n",
    "# Display the first few rows of the DataFrame in the notebook\n",
    "internal_clickstream.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c015ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the `internal_clickstream` DataFrame to a series of CSV files in `./results/article_links_csv/`\n",
    "# with `DataFrame.write.csv()`\n",
    "internal_clickstream\\\n",
    "    .write\\\n",
    "    .csv('article_links_csv', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d542098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the `internal_clickstream` DataFrame to a series of parquet files in `./results/article_links_parquet/`\n",
    "# with `DataFrame.write.parquet()`\n",
    "\n",
    "internal_clickstream\\\n",
    "    .write\\\n",
    "    .parquet('article_links_parquet', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the notebook's `SparkSession` and `SparkContext`\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b663c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SparkSession and sparkContext are stopped; the following line will throw \n",
    "# an error:\n",
    "clickstream.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
